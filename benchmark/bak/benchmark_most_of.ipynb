{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T06:27:59.447591Z",
     "start_time": "2024-03-30T06:27:54.703915500Z"
    }
   },
   "outputs": [],
   "source": [
    "from feature_select import *\n",
    "from pyod.utils.data import evaluate_print\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import experiment_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T06:28:01.613893300Z",
     "start_time": "2024-03-30T06:28:01.100290600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbait_nonclickbait 13201\n",
      "Corona_NLP 44955\n",
      "movie_review 30000\n"
     ]
    }
   ],
   "source": [
    "dataset_path,dataset_name = experiment_config.get_path_and_name()\n",
    "num_dataset = len(dataset_path)\n",
    "# load dataset\n",
    "df = [pd.read_json(dataset_path[i], lines=True) for i in range(num_dataset)]\n",
    "texts = [df[i]['text'].tolist() for i in range(num_dataset)]\n",
    "labels = [df[i]['label'].tolist() for i in range(num_dataset)]\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i], end=' ')\n",
    "    print(len(texts[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T06:28:03.946737500Z",
     "start_time": "2024-03-30T06:28:03.701738500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbait_nonclickbait (13201, 768)\n",
      "Corona_NLP (44955, 768)\n",
      "movie_review (30000, 768)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# feature selection\n",
    "features = [np.load('./feature/'+dataset_name[i]+'_feature.npy') for i in range(num_dataset)]\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i], end=' ')\n",
    "    print(features[i].shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T06:28:06.750020Z",
     "start_time": "2024-03-30T06:28:06.667020600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbait_nonclickbait  (8844, 768) (4357, 768) 8844 4357\n",
      "Corona_NLP  (30119, 768) (14836, 768) 30119 14836\n",
      "movie_review  (20100, 768) (9900, 768) 20100 9900\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = [], [], [], []\n",
    "for i in range(num_dataset):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(features[i], labels[i], test_size=0.33, random_state=42)\n",
    "    X_train.append(xtrain)\n",
    "    X_test.append(xtest)\n",
    "    y_train.append(ytrain)\n",
    "    y_test.append(ytest)\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i], end='  ')\n",
    "    print(X_train[i].shape, X_test[i].shape, len(y_train[i]), len(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T08:16:28.635658300Z",
     "start_time": "2024-03-27T08:09:47.014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbait_nonclickbait:\n",
      "\n",
      "On Training Data:\n",
      "KNN ROC:0.1872, precision @ rank n:0.0918\n",
      "\n",
      "On Test Data:\n",
      "KNN ROC:0.1864, precision @ rank n:0.0912\n",
      "\n",
      "Corona_NLP:\n",
      "\n",
      "On Training Data:\n",
      "KNN ROC:0.5542, precision @ rank n:0.1661\n",
      "\n",
      "On Test Data:\n",
      "KNN ROC:0.5597, precision @ rank n:0.1711\n",
      "\n",
      "movie_review:\n",
      "\n",
      "On Training Data:\n",
      "KNN ROC:0.4197, precision @ rank n:0.121\n",
      "\n",
      "On Test Data:\n",
      "KNN ROC:0.4273, precision @ rank n:0.1117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from pyod.models.knn import KNN\n",
    "def KNN_benchmark(X_train, X_test, y_train, y_test, KNN_Hyerparameters):\n",
    "    clf_name = 'KNN'\n",
    "    clf = KNN()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    y_train_scores = clf.decision_scores_  \n",
    "\n",
    "    y_test_scores = clf.decision_function(X_test) \n",
    "\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "KNN_hyerparameters = None\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    KNN_benchmark(X_train[i], X_test[i], y_train[i], y_test[i], KNN_hyerparameters)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T20:20:32.322838100Z",
     "start_time": "2024-03-25T20:20:22.628522100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbait_nonclickbait:\n",
      "\n",
      "On Training Data:\n",
      "ABOD ROC:0.2279, precision @ rank n:0.0894\n",
      "\n",
      "On Test Data:\n",
      "ABOD ROC:0.2309, precision @ rank n:0.0855\n",
      "\n",
      "Corona_NLP:\n",
      "\n",
      "On Training Data:\n",
      "ABOD ROC:0.5484, precision @ rank n:0.1587\n",
      "\n",
      "On Test Data:\n",
      "ABOD ROC:0.5547, precision @ rank n:0.1691\n",
      "\n",
      "movie_review:\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.abod import ABOD\n",
    "from pyod.utils.data import generate_data\n",
    "from pyod.utils.data import evaluate_print\n",
    "def ABOD_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1  # percentage of outliers\n",
    "    n_train = 200  # number of training points\n",
    "    n_test = 100  # number of testing points\n",
    "\n",
    "    # Generate sample data\n",
    "    \n",
    "    clf_name = 'ABOD'\n",
    "    clf = ABOD()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "    \n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    # for item in y_train_scores:\n",
    "    #     if np.isnan(item):\n",
    "    #         print('yes')\n",
    "    # print(len(y_train))\n",
    "    # print(len(y_train_scores))\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    ABOD_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T20:26:45.481591400Z",
     "start_time": "2024-03-25T20:24:51.257309400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.alad import ALAD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def ALAD_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    clf_name = 'ALAD'\n",
    "    clf = ALAD(epochs=100, latent_dim=2,\n",
    "               learning_rate_disc=0.0001,\n",
    "               learning_rate_gen=0.0001,\n",
    "               dropout_rate=0.2,\n",
    "               add_recon_loss=False,\n",
    "               lambda_recon_loss=0.05,\n",
    "               add_disc_zz_loss=True,\n",
    "               dec_layers=[75, 100],\n",
    "               enc_layers=[100, 75],\n",
    "               disc_xx_layers=[100, 75],\n",
    "               disc_zz_layers=[25, 25],\n",
    "               disc_xz_layers=[100, 75],\n",
    "               spectral_normalization=False,\n",
    "               activation_hidden_disc='tanh', activation_hidden_gen='tanh',\n",
    "               preprocessing=True, batch_size=200, contamination=contamination)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    ALAD_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T20:31:36.139099900Z",
     "start_time": "2024-03-25T20:27:29.105717700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.utils.data import evaluate_print\n",
    "def auto_encoder_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    clf_name = 'AutoEncoder'\n",
    "    clf = AutoEncoder(epochs=30, contamination=contamination)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    auto_encoder_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T20:34:06.155654Z",
     "start_time": "2024-03-25T20:33:47.421135500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.utils.data import evaluate_print\n",
    "def cblof_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    clf_name = 'CBLOF'\n",
    "    clf = CBLOF(random_state=42)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    cblof_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T20:38:08.319177500Z",
     "start_time": "2024-03-25T20:37:50.413958900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.cd import CD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def cd_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    clf_name = 'CBLOF'\n",
    "    clf = CBLOF(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    cd_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:46:39.261837400Z",
     "start_time": "2024-03-25T20:42:21.107498700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.cof import COF\n",
    "from pyod.utils.data import evaluate_print\n",
    "def cof_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    # train COF detector\n",
    "    clf_name = 'COF'\n",
    "    clf = COF(n_neighbors=30)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    cof_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T03:11:25.911496400Z",
     "start_time": "2024-03-26T00:27:09.595365300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.utils.data import evaluate_print\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.combination import aom, moa, average, maximization, median\n",
    "from pyod.utils.utility import standardizer\n",
    "def comb_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    # train COF detector\n",
    "    X_train_norm, X_test_norm = standardizer(X_train, X_test)\n",
    "\n",
    "    n_clf = 20  # number of base detectors\n",
    "\n",
    "    # Initialize 20 base detectors for combination\n",
    "    k_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140,\n",
    "              150, 160, 170, 180, 190, 200]\n",
    "\n",
    "    train_scores = np.zeros([X_train.shape[0], n_clf])\n",
    "    test_scores = np.zeros([X_test.shape[0], n_clf])\n",
    "\n",
    "    print('Combining {n_clf} kNN detectors'.format(n_clf=n_clf))\n",
    "\n",
    "    for i in range(n_clf):\n",
    "        k = k_list[i]\n",
    "\n",
    "        clf = KNN(n_neighbors=k, method='largest')\n",
    "        clf.fit(X_train_norm)\n",
    "\n",
    "        train_scores[:, i] = clf.decision_scores_\n",
    "        test_scores[:, i] = clf.decision_function(X_test_norm)\n",
    "\n",
    "    # Decision scores have to be normalized before combination\n",
    "    train_scores_norm, test_scores_norm = standardizer(train_scores,\n",
    "                                                       test_scores)\n",
    "    # Combination by average\n",
    "    y_by_average = average(train_scores_norm)\n",
    "    evaluate_print('Combination by Average train', y_train, y_by_average)\n",
    "    y_by_average = average(test_scores_norm)\n",
    "    evaluate_print('Combination by Average test', y_test, y_by_average)\n",
    "\n",
    "    # Combination by max\n",
    "    y_by_maximization = maximization(train_scores_norm)\n",
    "    evaluate_print('Combination by Maximization train', y_train, y_by_maximization)\n",
    "    y_by_maximization = maximization(test_scores_norm)\n",
    "    evaluate_print('Combination by Maximization test', y_test, y_by_maximization)\n",
    "\n",
    "    # Combination by median\n",
    "    y_by_median = median(train_scores_norm)\n",
    "    evaluate_print('Combination by Median train', y_train, y_by_median)\n",
    "    y_by_median = median(test_scores_norm)\n",
    "    evaluate_print('Combination by Median test', y_test, y_by_median)\n",
    "\n",
    "    # Combination by aom\n",
    "    y_by_aom = aom(train_scores_norm, n_buckets=5)\n",
    "    evaluate_print('Combination by AOM train', y_train, y_by_aom)\n",
    "    y_by_aom = aom(test_scores_norm, n_buckets=5)\n",
    "    evaluate_print('Combination by AOM test', y_test, y_by_aom)\n",
    "\n",
    "    # Combination by moa\n",
    "    y_by_moa = moa(train_scores_norm, n_buckets=5)\n",
    "    evaluate_print('Combination by MOA train', y_train, y_by_moa)\n",
    "    y_by_moa = moa(test_scores_norm, n_buckets=5)\n",
    "    evaluate_print('Combination by MOA test', y_test, y_by_moa)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    comb_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T03:23:51.173826600Z",
     "start_time": "2024-03-26T03:22:55.220986600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.copod import COPOD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def cof_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    # train COF detector\n",
    "    # train COPOD detector\n",
    "    clf_name = 'COPOD'\n",
    "    clf = COPOD()\n",
    "\n",
    "    # you could try parallel version as well.\n",
    "    # clf = COPOD(n_jobs=2)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    cof_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T04:39:19.349914400Z",
     "start_time": "2024-03-26T04:39:03.283880300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def cof_benchmark(X_train, X_test, y_train, y_test):\n",
    "    use_ae = False  # hyperparameter for use ae architecture instead of simple NN\n",
    "    random_state = 10  # if C is set to None use random_state\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    # train COF detector\n",
    "    # train COPOD detector\n",
    "    clf_name = 'DeepSVDD'\n",
    "    clf = DeepSVDD(use_ae=use_ae, epochs=5, contamination=contamination,\n",
    "                   random_state=random_state)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    cof_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T04:47:31.471015Z",
     "start_time": "2024-03-26T04:42:25.081821Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.dif import DIF\n",
    "from pyod.utils.data import evaluate_print\n",
    "def dif_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    # train COF detector\n",
    "    # train COPOD detector\n",
    "    clf_name = 'DIF'\n",
    "    clf = DIF()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    dif_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T06:36:44.663591500Z",
     "start_time": "2024-03-27T06:35:37.492390100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.ecod import ECOD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def ecod_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    # train COF detector\n",
    "    # train COPOD detector\n",
    "    clf_name = 'ECOD'\n",
    "    clf = ECOD()\n",
    "\n",
    "    # you could try parallel version as well.\n",
    "    # clf = ECOD(n_jobs=2)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    ecod_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T05:00:50.054857400Z",
     "start_time": "2024-03-26T04:57:55.161797400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.utils.data import evaluate_print\n",
    "def FeatureBagging_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    # train COF detector\n",
    "    # train COPOD detector\n",
    "    clf_name = 'FeatureBagging'\n",
    "    clf = FeatureBagging(check_estimator=False)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    FeatureBagging_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T05:11:13.320052600Z",
     "start_time": "2024-03-26T05:08:16.348063700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.gmm import GMM\n",
    "from pyod.utils.data import evaluate_print\n",
    "def gmm_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train AutoEncoder detector\n",
    "    # train COF detector\n",
    "    # train COPOD detector\n",
    "    clf_name = \"GMM\"\n",
    "    clf = GMM(n_components=4)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    gmm_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T05:16:09.845937700Z",
     "start_time": "2024-03-26T05:16:02.770628900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.hbos import HBOS\n",
    "from pyod.utils.data import evaluate_print\n",
    "def hbos_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train HBOS detector\n",
    "    clf_name = 'HBOS'\n",
    "    clf = HBOS()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    hbos_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T05:22:09.309307500Z",
     "start_time": "2024-03-26T05:22:06.247385500Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def IForest_benchmark_pca(X_train, X_test, y_train, y_test):\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_norm = scaler.fit_transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    # Reduce the data into two principal components for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_train_pca = pca.fit_transform(X_train_norm)\n",
    "    X_test_pca = pca.transform(X_test_norm)\n",
    "\n",
    "    # Train IForest detector\n",
    "    clf_name = 'IForest'\n",
    "    clf = IForest()  # Add hyperparameters if necessary\n",
    "    clf.fit(X_train_pca)\n",
    "\n",
    "    # Get the prediction on the training data\n",
    "    y_train_pred = clf.predict(X_train_pca)\n",
    "    y_train_scores = clf.decision_function(X_train_pca)\n",
    "\n",
    "    # Get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test_pca)\n",
    "    y_test_scores = clf.decision_function(X_test_pca)\n",
    "\n",
    "    # Evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "    \n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are defined and properly preprocessed\n",
    "IForest_Hyerparameters = None\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    IForest_benchmark_pca(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T06:39:26.998478200Z",
     "start_time": "2024-03-27T06:38:25.451906100Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyod.models.inne import INNE\n",
    "\n",
    "from pyod.utils.data import evaluate_print\n",
    "def inne_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train HBOS detector\n",
    "    clf_name = 'INNE'\n",
    "    clf = INNE(contamination=contamination, max_samples=4)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    inne_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T07:56:52.512405100Z",
     "start_time": "2024-03-27T06:54:15.664444500Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyod.models.kde import KDE\n",
    "from pyod.utils.data import evaluate_print\n",
    "def kde_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train HBOS detector\n",
    "    clf_name = 'kde'\n",
    "    clf = KDE()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    kde_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T05:40:20.620232800Z",
     "start_time": "2024-03-26T05:39:57.329455500Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "from pyod.utils.data import evaluate_print\n",
    "def knn_mahalanobis_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train kNN detector with mahalanobis distance\n",
    "    clf_name = 'KNN (mahalanobis distance)'\n",
    "    # calculate covariance for mahalanobis distance\n",
    "    X_train_cov = np.cov(X_train, rowvar=False)\n",
    "    clf = KNN(algorithm='auto', metric='mahalanobis',\n",
    "              metric_params={'V': X_train_cov})\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    knn_mahalanobis_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:25:50.599607500Z",
     "start_time": "2024-03-26T10:11:07.795836400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.kpca import KPCA\n",
    "from pyod.utils.data import evaluate_print\n",
    "def kpca_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    clf_name = \"KPCA\"\n",
    "    clf = KPCA()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    kpca_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T10:05:57.468648500Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from pyod.models.lmdd import LMDD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def imdd_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    # train LMDD detector\n",
    "    clf_name = 'LMDD'\n",
    "    clf = LMDD(random_state=42)\n",
    "    print(11)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    imdd_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T10:05:50.853947Z",
     "start_time": "2024-03-27T10:05:50.851946400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.loci import LOCI\n",
    "from pyod.utils.data import evaluate_print\n",
    "def loci_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train LMDD detector\n",
    "    # train LOCI detector\n",
    "    clf_name = 'LOCI'\n",
    "    clf = LOCI()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    loci_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.044998Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.loda import LODA\n",
    "from pyod.utils.data import evaluate_print\n",
    "def loda_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train LOCI detector\n",
    "    clf_name = 'LODA'\n",
    "    clf = LODA()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    loda_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.048001600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.lof import LOF\n",
    "from pyod.utils.data import evaluate_print\n",
    "def lof_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train LOF detector\n",
    "    clf_name = 'LOF'\n",
    "    clf = LOF()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    lof_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T06:13:50.049997900Z",
     "start_time": "2024-03-27T06:13:50.049997900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.lscp import LSCP\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.utils.data import evaluate_print\n",
    "def lcsp_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train lscp\n",
    "    clf_name = 'LSCP'\n",
    "    detector_list = [LOF(n_neighbors=15), LOF(n_neighbors=20),\n",
    "                     LOF(n_neighbors=25), LOF(n_neighbors=35)]\n",
    "    clf = LSCP(detector_list, random_state=42)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    lcsp_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.052002300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.lunar import LUNAR\n",
    "from pyod.utils.data import evaluate_print\n",
    "def lunar_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train LUNAR detector\n",
    "    clf_name = 'LUNAR'\n",
    "    clf = LUNAR()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    lunar_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.053001800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.mad import MAD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def mad_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train MAD detector\n",
    "    clf_name = 'MAD'\n",
    "    clf = MAD(threshold=3.5)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    mad_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.055001800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.mcd import MCD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def mcd_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train LOF detector\n",
    "    clf_name = 'MCD'\n",
    "    clf = MCD()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    mcd_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.056001600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.mo_gaal import MO_GAAL\n",
    "from pyod.utils.data import evaluate_print\n",
    "def mo_gaal_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train MO_GAAL detector\n",
    "    clf_name = 'MO_GAAL'\n",
    "    clf = MO_GAAL(k=3, stop_epochs=2, contamination=contamination)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    mo_gaal_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.058001900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.utils.data import evaluate_print\n",
    "def ocsvm_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train one_class_svm detector\n",
    "    clf_name = 'OneClassSVM'\n",
    "    clf = OCSVM()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    ocsvm_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T06:13:50.062001800Z",
     "start_time": "2024-03-27T06:13:50.060001500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.pca import PCA\n",
    "from pyod.utils.data import evaluate_print\n",
    "def pca_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train PCA detector\n",
    "    clf_name = 'PCA'\n",
    "    clf = PCA(n_components=3)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    pca_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.061001800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.qmcd import QMCD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def qmcd_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train QMCD detector\n",
    "    clf_name = 'QMCD'\n",
    "    clf = QMCD()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(np.append(X_test, y_test.reshape(-1, 1), axis=1))  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(np.append(X_test, y_test.reshape(-1, 1), axis=1))  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    qmcd_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T06:13:50.064001600Z",
     "start_time": "2024-03-27T06:13:50.062001800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.rgraph import RGraph\n",
    "from pyod.utils.data import evaluate_print\n",
    "def rgraph_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train R-graph detector\n",
    "    clf_name = 'R-graph'\n",
    "    clf = RGraph(n_nonzero=100, transition_steps=20, gamma=50, gamma_nz=False,\n",
    "             tau=1, preprocessing=True, active_support=False,\n",
    "             blocksize_test_data=20,\n",
    "             algorithm='lasso_lars', maxiter=100, verbose=1)\n",
    "\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    rgraph_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T06:47:59.685510500Z",
     "start_time": "2024-03-30T06:42:13.472882800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.rod import ROD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def rod_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train ROD detector\n",
    "    clf_name = 'ROD'\n",
    "    clf = ROD()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    rod_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.065001500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.sampling import Sampling\n",
    "from pyod.utils.data import evaluate_print\n",
    "def sampling_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train kNN detector\n",
    "    clf_name = \"Sampling\"\n",
    "    clf = Sampling()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    sampling_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.066001500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.so_gaal import SO_GAAL\n",
    "from pyod.utils.data import evaluate_print\n",
    "def so_gaal_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train SO_GAAL detector\n",
    "    clf_name = 'SO_GAAL'\n",
    "    clf = SO_GAAL(stop_epochs=2, contamination=contamination)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    so_gaal_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.066998400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.so_gaal import SO_GAAL\n",
    "from pyod.utils.data import evaluate_print\n",
    "def so_gaal_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train SO_GAAL detector\n",
    "    clf_name = 'SO_GAAL'\n",
    "    clf = SO_GAAL(stop_epochs=2, contamination=contamination)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    so_gaal_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.068997700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.sod import SOD\n",
    "from pyod.utils.data import evaluate_print\n",
    "def sod_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # thus, higher precision is expected in higher dimensions\n",
    "    clf_name = 'SOD'\n",
    "    clf = SOD()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    sod_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.069998Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.sos import SOS\n",
    "from pyod.utils.data import evaluate_print\n",
    "def sos_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # thus, higher precision is expected in higher dimensions\n",
    "    clf_name = 'SOS'\n",
    "    clf = SOS()\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    sos_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.070997800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.suod import SUOD\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.copod import COPOD\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.utils.data import evaluate_print\n",
    "def suod_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "    # train SUOD\n",
    "    clf_name = 'SUOD'\n",
    "\n",
    "    # initialized a group of outlier detectors for acceleration\n",
    "    detector_list = [LOF(n_neighbors=15), LOF(n_neighbors=20),\n",
    "                     LOF(n_neighbors=25), LOF(n_neighbors=35),\n",
    "                     COPOD(), IForest(n_estimators=100),\n",
    "                     IForest(n_estimators=200)]\n",
    "\n",
    "    # decide the number of parallel process, and the combination method\n",
    "    clf = SUOD(base_estimators=detector_list, n_jobs=2, combination='average',\n",
    "               verbose=False)\n",
    "\n",
    "    # or to use the default detectors\n",
    "    # clf = SUOD(n_jobs=2, combination='average',\n",
    "    #            verbose=False)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    suod_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.071997900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyod.models.vae import VAE\n",
    "from pyod.utils.data import evaluate_print\n",
    "def vae_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "   # train VAE detector (Beta-VAE)\n",
    "    clf_name = 'VAE'\n",
    "    clf = VAE(epochs=30, contamination=contamination, gamma=0.8, capacity=0.2)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    vae_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-27T06:13:50.072997600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.utils.data import generate_data\n",
    "from pyod.utils.data import evaluate_print\n",
    "def xgbod_benchmark(X_train, X_test, y_train, y_test):\n",
    "    contamination = 0.1\n",
    "    \n",
    "   # train XGBOD detector\n",
    "    clf_name = 'XGBOD'\n",
    "    clf = XGBOD(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "    # get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "    # evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "for i in range(num_dataset):\n",
    "    print(dataset_name[i]+':')\n",
    "    xgbod_benchmark(X_train[i], X_test[i], y_train[i], y_test[i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
